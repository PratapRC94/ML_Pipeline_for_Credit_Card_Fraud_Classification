{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "32e24621",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the libraries and functions necessary for the pipeline\n",
    "\n",
    "import joblib\n",
    "import pandas as pd\n",
    "from numpy import inf, nan, where\n",
    "from sklearn.metrics import roc_auc_score, classification_report\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e6992bd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create constant variables containing the groups of variables \n",
    "# that need specific transformations or are used as input\n",
    "\n",
    "# The features needed to create all input features\n",
    "FEATURES_ALL = [\n",
    "    'new_customer', 'application_date', 'income_verification', 'language',\n",
    "    'date_of_birth', 'gender', 'country', 'loan_amount', 'county', 'city',\n",
    "    'use_of_loan', 'education', 'marital_status', 'nr_dependants',\n",
    "    'employment_status', 'employment_duration', 'employment_position',\n",
    "    'work_experience', 'occupation', 'home_ownership',\n",
    "    'income_from_employer', 'income_from_pension',\n",
    "    'income_from_family_allowance', 'income_from_social_welfare',\n",
    "    'income_from_leave_pay', 'income_from_child_support', 'income_other',\n",
    "    'nr_debt_items', 'total_debt', 'credit_score_1', 'credit_score_2',\n",
    "    'credit_score_3', 'credit_score_4', 'nr_previous_loans',\n",
    "    'amount_previous_loans', 'previous_repayments',\n",
    "    'previous_early_repayments', 'previous_early_repayments_count'\n",
    "]\n",
    " \n",
    "# Input features to the model (important: mind \n",
    "# the order used when training the model)\n",
    "FEATURES_INPUT = [\n",
    "    'new_customer', 'income_verification', 'language', 'gender', 'country',\n",
    "    'loan_amount', 'use_of_loan', 'education', 'marital_status',\n",
    "    'nr_dependants', 'employment_status', 'employment_duration',\n",
    "    'work_experience', 'occupation', 'home_ownership',\n",
    "    'income_from_employer', 'income_from_pension',\n",
    "    'income_from_family_allowance', 'income_from_social_welfare',\n",
    "    'income_from_leave_pay', 'income_from_child_support', 'income_other',\n",
    "    'nr_debt_items', 'total_debt', 'credit_score_1', 'credit_score_2',\n",
    "    'credit_score_3', 'credit_score_4', 'nr_previous_loans',\n",
    "    'amount_previous_loans', 'previous_repayments',\n",
    "    'previous_early_repayments', 'previous_early_repayments_count',\n",
    "    'total_income', 'dti', 'cash', 'age', 'dow', 'dom', 'month', 'hour',\n",
    "]\n",
    " \n",
    "FEATURES_INCOME = [\n",
    "    'income_from_employer',\n",
    "    'income_from_pension',\n",
    "    'income_from_family_allowance',\n",
    "    'income_from_social_welfare',\n",
    "    'income_from_leave_pay',\n",
    "    'income_from_child_support',\n",
    "    'income_other',\n",
    "]\n",
    " \n",
    "FEATURES_ENCODE = [\n",
    "    'income_verification',\n",
    "    'language',\n",
    "    'gender',\n",
    "    'country',\n",
    "    'use_of_loan',\n",
    "    'education',\n",
    "    'marital_status',\n",
    "    'employment_status',\n",
    "    'employment_duration',\n",
    "    'work_experience',\n",
    "    'occupation',\n",
    "    'home_ownership',\n",
    "    'credit_score_1',\n",
    "    'credit_score_2',\n",
    "    'credit_score_3',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "35f7a0ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hard-code the dictionaries with the imputation parameters and the frequent categories \n",
    "# learned during the development of the credit risk model\n",
    "\n",
    "IMPUTATION_DICT = {\n",
    "    'nr_dependants': -1,\n",
    "    'credit_score_4': -1,\n",
    "    'previous_repayments': -1,\n",
    "    'previous_early_repayments': -1,\n",
    "    'gender': 'missing',\n",
    "    'education': 'missing',\n",
    "    'marital_status': 'missing',\n",
    "    'employment_status': 'missing',\n",
    "    'employment_duration': 'missing',\n",
    "    'work_experience': 'missing',\n",
    "    'occupation': 'missing',\n",
    "    'home_ownership': 'missing',\n",
    "    'credit_score_1': 'missing',\n",
    "    'credit_score_2': 'missing',\n",
    "    'credit_score_3': 'missing',\n",
    "}\n",
    " \n",
    "# Frequent categories\n",
    " \n",
    "FREQUENT_CAT_DICT = {\n",
    "    'language': ['estonian', 'finnish', 'spanish', 'russian'],\n",
    "    'use_of_loan': ['unknown', 'other', 'home_improvement', 'loan_consolidation'],\n",
    "    'occupation': ['missing', 'other', 'retail'],\n",
    "    'home_ownership': ['owner',\n",
    "                       'tenant_furnished',\n",
    "                       'living_with_parents',\n",
    "                       'mortgage',\n",
    "                       'tenant_unfurnished'],\n",
    "    'credit_score_1': ['missing', 'M', 'M1'],\n",
    "    'credit_score_2': ['missing', 'B'],\n",
    "    'credit_score_3': ['missing', 'RL2']\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2807d426",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a function containing the complete sequence of feature transformations and creation:\n",
    "\n",
    "def feature_engineering_pipe(df):\n",
    "    \n",
    "    # Make a copy of the input features\n",
    "    df = df[FEATURES_ALL].copy()\n",
    "    \n",
    "    # Impute missing data\n",
    "    df.fillna(IMPUTATION_DICT, inplace=True)\n",
    "    \n",
    "    # Create income related variables\n",
    "    df['total_income'] = df[FEATURES_INCOME].sum(axis=1)\n",
    "    \n",
    "    df[\"dti\"] = df[\"total_debt\"].div(df[\"total_income\"])\n",
    "    df[\"dti\"].replace([inf, nan], 0, inplace=True)\n",
    "    \n",
    "    df[\"cash\"] = df[\"total_income\"].sub(df[\"total_debt\"])\n",
    " \n",
    "    # Create datetime related features\n",
    "    df[\"age\"] = ((pd.to_datetime(df[\"application_date\"]) -\n",
    "                  pd.to_datetime(df[\"date_of_birth\"])).dt.days/365).astype(int)\n",
    "        \n",
    "    df[\"application_date\"] = pd.to_datetime(df[\"application_date\"])    \n",
    "    df[\"dow\"] = df[\"application_date\"].dt.day_of_week\n",
    "    df[\"dom\"] = df[\"application_date\"].dt.day\n",
    "    df[\"month\"] = df[\"application_date\"].dt.month\n",
    "    df[\"hour\"] = df[\"application_date\"].dt.hour\n",
    "        \n",
    "    # Group infrequent labels\n",
    "    for var in FREQUENT_CAT_DICT.keys():\n",
    "        df[var] = where(df[var].isin(\n",
    "            FREQUENT_CAT_DICT[var]), df[var], \"Rare\")\n",
    " \n",
    "    # Encode categorical variables\n",
    "    df[FEATURES_ENCODE] = enc.transform(df[FEATURES_ENCODE])\n",
    " \n",
    "    # Return features in the order in which they were passed to the \n",
    "    # model during training\n",
    "    \n",
    "    return df[FEATURES_INPUT]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eb0dced",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the encoder and the lightGBM\n",
    "\n",
    "enc = joblib.load(\"encoder.pkl\")\n",
    "gbm = joblib.load(\"lightGBM.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "365a45ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the pipeline! Load and split the data\n",
    "\n",
    "df = pd.read_csv(\"loan_data.csv\", low_memory=False)\n",
    " \n",
    "seed=10\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    df.drop(\"default\", axis=1),\n",
    "    df[\"default\"],\n",
    "    test_size=0.20,\n",
    "    random_state=seed,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "398d1494",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove observations from customers who entered the date of birth wrongly and therefore seem to be under 18\n",
    "\n",
    "train_index = [\n",
    "    30394,  8998,  2634, 20910,  2560,  2434,  1148,  2448,   889,\n",
    "    11656, 18764,  1166, 11647,  2565, 18823,  2475,  2403,  8802,\n",
    "    9081,  2479, 11557, 19035,  1132,  2439, 30369,  2423,  1174,\n",
    "    20932, 20892,   919, 18765,  2589,  8770, 11386, 20941, 20895,\n",
    "    956, 11555,   930, 20925,  2509,\n",
    "]\n",
    " \n",
    "test_index = [\n",
    "    915, 30378, 18914, 922, 18842, 8939, 2486, 30412, 11676, 1137,\n",
    "    2440, 2447\n",
    "]\n",
    " \n",
    "X_train = X_train.drop(index=train_index)\n",
    "X_test = X_test.drop(index=test_index)\n",
    " \n",
    "y_train = y_train.drop(index=train_index)\n",
    "y_test = y_test.drop(index=test_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eddb789d",
   "metadata": {},
   "source": [
    "> Note that you do not make this part of the pipeline, because in the platform, customers that enter their date of birth wrongly or are otherwise under 18, would be rejected automatically and not passed on to the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "776699aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain the input features for the lightGBM using the pipeline\n",
    "\n",
    "X_train_t = feature_engineering_pipe(X_train)\n",
    "X_test_t = feature_engineering_pipe(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d2e90f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To corroborate the pipeline functionality, evaluate the ROC-AUC of the returned probabilities\n",
    "\n",
    "pred_train = gbm.predict_proba(X_train_t)[:, 1]\n",
    "pred_test = gbm.predict_proba(X_test_t)[:, 1]\n",
    " \n",
    "roc_train = roc_auc_score(y_train, pred_train)\n",
    "roc_test = roc_auc_score(y_test, pred_test)\n",
    " \n",
    "print(f\"Train set roc-auc: {roc_train}\")\n",
    "print(f\"Eval set roc-auc: {roc_test}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3b7fda3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now produce the full classification report for the train and test sets\n",
    "\n",
    "pred_train = gbm.predict(X_train_t)\n",
    "pred_test = gbm.predict(X_test_t)\n",
    " \n",
    "cr_train = classification_report(y_train, pred_train)\n",
    "cr_test = classification_report(y_test, pred_test)\n",
    " \n",
    "print(f\"Train set:\\n {cr_train}\")\n",
    "print(f\"Eval set:\\n {cr_test}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf63fdb6",
   "metadata": {},
   "source": [
    "### Evaluate a cohort of recent customers\n",
    "\n",
    "We've got another data set with information about our most recent customer applications. To be fully confident that the model continues to have good performance on recent customer cohorts, could you please obtain the predictions of credit risk for these customers as well?\n",
    "\n",
    "We've placed the raw data, as it comes through our platform, in a file called latest_customers.csv at the root of your workspace. The dataset also contains the target variable default.\n",
    "\n",
    "Please pass this data through your pipeline, obtain the model predictions, and then evaluate the model's performance.\n",
    "\n",
    "One thing to notice is that, as these customers have just gotten their loans, we do not have enough history to accurately determine if they are going to default in the near future. As a result, we most likely labeled customers as \"no default\", even though they may likely default in the coming months. Hence, if the performance metrics are a bit lower than observed, don't worry. As the maturity of the loans develops, we will be able to better assess their performance in the coming months."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c03814d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load latest cohort of customers\n",
    "\n",
    "df = pd.read_csv(\"latest_customers.csv\", low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c1565ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Capture the target in a separate variable\n",
    "\n",
    "df_target = df[\"default\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf0950ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the input features for the model\n",
    "\n",
    "df_t = feature_engineering_pipe(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73b3a6b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain the predictions and determine the ROC-AUC\n",
    "\n",
    "pred_df = gbm.predict_proba(df_t)[:, 1]\n",
    "roc_df = roc_auc_score(df_target, pred_df)\n",
    "print(f\"roc-auc: {roc_df}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d156dfd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, classify the customers and obtain the full classification report\n",
    "\n",
    "pred_df = gbm.predict(df_t)\n",
    "cr_df = classification_report(df_target, pred_df)\n",
    "print(f\"New data set:\\n {cr_df}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3393b6d4",
   "metadata": {},
   "source": [
    "> As expected, the performance metrics are a bit lower than those observed in the previous task. This is probably because some customers that are flagged as no-default, will likely default in the coming months. This is quite common when creating and assessing credit risk models."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
